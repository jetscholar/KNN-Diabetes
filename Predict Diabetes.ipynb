{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9fde4c1",
   "metadata": {},
   "source": [
    "## Predict Diabetes with K-NN\n",
    "\n",
    "### Objective: Predict whether a person will be diagnosed with diabetes or not\n",
    "\n",
    "- Dataset of 768 diagnosed people with or without diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67db06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4631cc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of DF:  768\n",
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"diabetes.csv\")\n",
    "print(\"Length of DF: \", len(data))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df97a7f",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "- Values for 'Glucose' and 'Blood Pressure' etc. cannot be accepted as zeros (why?)\n",
    "- Replace these columns with the mean of the column\n",
    "- Replace with average since it is the most common for a person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21807ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      148\n",
      "1       85\n",
      "2      183\n",
      "3       89\n",
      "4      137\n",
      "      ... \n",
      "763    101\n",
      "764    122\n",
      "765    121\n",
      "766    126\n",
      "767     93\n",
      "Name: Glucose, Length: 768, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['Glucose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cc62700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the zeroes\n",
    "zero_not_accepted = ['Glucose', 'BloodPressure', 'SkinThickness', 'BMI', 'Insulin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d71d97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the data and clean it using pandas and numpy tools\n",
    "for column in zero_not_accepted:\n",
    "    data[column] = data[column].replace(0, np.NaN)\n",
    "    mean = int(data[column].mean(skipna=True))\n",
    "    data[column] = data[column].replace(np.NaN, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a0d24e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      148.0\n",
      "1       85.0\n",
      "2      183.0\n",
      "3       89.0\n",
      "4      137.0\n",
      "       ...  \n",
      "763    101.0\n",
      "764    122.0\n",
      "765    121.0\n",
      "766    126.0\n",
      "767     93.0\n",
      "Name: Glucose, Length: 768, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data['Glucose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70af6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "x = data.iloc[:, 0:8] # keep all rows, except column 8 (attributes)\n",
    "y = data.iloc[:, 8] # Label - column 8\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e62f13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute scaling\n",
    "sc_x = StandardScaler() # sets all data between -1 and 1\n",
    "x_train = sc_x.fit_transform(x_train) # train data is fit\n",
    "x_test = sc_x.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9469ad",
   "metadata": {},
   "source": [
    "### Determining the model variables\n",
    "\n",
    "- The hyperparameter K should be an odd number (why?)\n",
    "- Use the square root of the length of the y_test set to pick K\n",
    "- p is the 'diabetic or not'\n",
    "- Euclidean is the method of measuring the distance between neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac8fd11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.409673645990857"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the hyperparameter, K\n",
    "import math\n",
    "print(len(y_test))\n",
    "math.sqrt(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b136351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(metric=&#x27;euclidean&#x27;, n_neighbors=11)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(metric=&#x27;euclidean&#x27;, n_neighbors=11)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(metric='euclidean', n_neighbors=11)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model: Init K-NN\n",
    "classifier = KNeighborsClassifier(n_neighbors=11, p=2, metric='euclidean')\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38b3e950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the test results\n",
    "y_pred = classifier.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28728e",
   "metadata": {},
   "source": [
    "### The Confusion Matrix Analysis Tool\n",
    "\n",
    "In machine learning, the confusion matrix is a table that is used to evaluate the performance of a classification model, such as a K-Nearest Neighbors (KNN) model. It provides a comprehensive summary of the predictions made by the model on a test dataset, compared to the actual ground truth labels. The confusion matrix has four important metrics: True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).\n",
    "\n",
    "1. True Positives (TP): The number of instances that are correctly predicted as positive (belong to the positive class).\n",
    "\n",
    "2. False Positives (FP): The number of instances that are incorrectly predicted as positive but actually belong to the negative class.\n",
    "\n",
    "3. True Negatives (TN): The number of instances that are correctly predicted as negative (belong to the negative class).\n",
    "\n",
    "4. False Negatives (FN): The number of instances that are incorrectly predicted as negative but actually belong to the positive class.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Actual</th>\n",
    "        <th>Predicted Positive</th>\n",
    "        <th>Predicted Negative</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Actual Positive </td>\n",
    "        <td>TP</td>\n",
    "        <td>FN</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Actual Negative </td>\n",
    "        <td>FP</td>\n",
    "        <td>TN</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Here's how to interpret the confusion matrix:\n",
    "\n",
    "1. TP (True Positives): The model correctly predicted instances that belong to the positive class.\n",
    "\n",
    "2. FN (False Negatives): The model incorrectly predicted instances as negative when they actually belong to the positive class.\n",
    "\n",
    "3. FP (False Positives): The model incorrectly predicted instances as positive when they actually belong to the negative class.\n",
    "\n",
    "4. TN (True Negatives): The model correctly predicted instances that belong to the negative class.\n",
    "\n",
    "Using these values, we can compute various evaluation metrics, such as **accuracy**, precision, recall (sensitivity), specificity, and **F1-score**, which help us assess the performance of the KNN model on the test data.\n",
    "\n",
    "- Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "- Precision = TP / (TP + FP)\n",
    "- Recall (Sensitivity) = TP / (TP + FN)\n",
    "- Specificity = TN / (TN + FP)\n",
    "- F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "A good KNN model will have a high accuracy, precision, recall, specificity, and F1-Score, indicating that it is making correct predictions for both positive and negative classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7c98b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[94 13]\n",
      " [15 32]]\n",
      "0.6956521739130436\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model with the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm)\n",
    "print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cd26bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9d779",
   "metadata": {},
   "source": [
    "### To interpret the results in the confusion matrix \n",
    "[[94, 13], \n",
    "[15, 32]]\n",
    "\n",
    "```\n",
    "             Predicted Positive    Predicted Negative\n",
    "Actual Positive        94                13\n",
    "Actual Negative        15                32\n",
    "```\n",
    "\n",
    "Here's how to interpret the values:\n",
    "\n",
    "1. True Positives (TP): The number of instances that are correctly predicted as positive. In this case, there are 94 true positives, meaning 94 instances were correctly classified as positive (belong to the positive class).\n",
    "\n",
    "2. False Positives (FP): The number of instances that are incorrectly predicted as positive but actually belong to the negative class. In this case, there are 13 false positives, meaning 13 instances were incorrectly classified as positive when they actually belong to the negative class.\n",
    "\n",
    "3. True Negatives (TN): The number of instances that are correctly predicted as negative. In this case, there are 32 true negatives, meaning 32 instances were correctly classified as negative (belong to the negative class).\n",
    "\n",
    "4. False Negatives (FN): The number of instances that are incorrectly predicted as negative but actually belong to the positive class. In this case, there are 15 false negatives, meaning 15 instances were incorrectly classified as negative when they actually belong to the positive class.\n",
    "\n",
    "Based on these values, we can calculate various evaluation metrics to assess the performance of the classification model:\n",
    "\n",
    "1. Accuracy: It is the overall correctness of the model's predictions and is calculated as (TP + TN) / (TP + FP + TN + FN). In this case, accuracy = (94 + 32) / (94 + 13 + 15 + 32) ≈ 0.8095 or 80.95%.\n",
    "\n",
    "2. Precision: It measures the accuracy of the positive predictions and is calculated as TP / (TP + FP). In this case, precision = 94 / (94 + 13) ≈ 0.8785 or 87.85%.\n",
    "\n",
    "3. Recall (Sensitivity): It measures the ability of the model to correctly identify positive instances and is calculated as TP / (TP + FN). In this case, recall = 94 / (94 + 15) ≈ 0.8621 or 86.21%.\n",
    "\n",
    "4. F1-Score: It is the harmonic mean of precision and recall and is given by 2 * (Precision * Recall) / (Precision + Recall). In this case, F1-Score = 2 * (0.8785 * 0.8621) / (0.8785 + 0.8621) ≈ 0.8702 or 87.02%.\n",
    "\n",
    "Our model has a relatively good accuracy, precision, recall, and F1-Score, indicating it performs reasonably well on the given test dataset. However, further analysis and comparison with other models may be necessary for a comprehensive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54009bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25d8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
